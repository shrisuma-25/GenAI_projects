{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d99e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key is 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\n",
      "ClassifierAgent ran\n",
      "state: {'question': ''}\n",
      "response = ChatCompletion(id='chatcmpl-CDFBwYvH05gpyoS5Fo56bdKn88YNs', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"faq_answer\": null,\\n  \"checkout_info\": null\\n}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1757272696, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_7c233bf9d1', usage=CompletionUsage(completion_tokens=16, prompt_tokens=56, total_tokens=72, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "Raw response: {\n",
      "  \"faq_answer\": null,\n",
      "  \"checkout_info\": null\n",
      "}\n",
      "Parsed response: {'faq_answer': None, 'checkout_info': None}\n",
      "CheckoutAgent ran\n",
      "FAQAgent ran\n",
      "FAQAgent state {'question': '', 'faq_answer': None, 'checkout_info': None}\n",
      "ResponseAgent ran\n",
      "ChatCompletion(id='chatcmpl-CDFBxn54sBolUNvtjwTDMinq0J0gX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='In our library’s quiet domain,  \\nRules are set to ease your gain.  \\nPlease observe each guideline clear,  \\nTo make your visit smooth and dear.\\n\\nWhile checkout info you did not ask,  \\nKnow books are yours upon the task.  \\nBorrow with care, and bring them back,  \\nRespect the rules — no need to track.\\n\\nSo, step inside, enjoy, explore,  \\nFollowing rules ensures much more.  \\nA space for all, both old and new,  \\nWith kindness, we serve you through.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1757272697, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_7c233bf9d1', usage=CompletionUsage(completion_tokens=105, prompt_tokens=65, total_tokens=170, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "\n",
      "--- Final Answer ---\n",
      "In our library’s quiet domain,  \n",
      "Rules are set to ease your gain.  \n",
      "Please observe each guideline clear,  \n",
      "To make your visit smooth and dear.\n",
      "\n",
      "While checkout info you did not ask,  \n",
      "Know books are yours upon the task.  \n",
      "Borrow with care, and bring them back,  \n",
      "Respect the rules — no need to track.\n",
      "\n",
      "So, step inside, enjoy, explore,  \n",
      "Following rules ensures much more.  \n",
      "A space for all, both old and new,  \n",
      "With kindness, we serve you through.\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Optional\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "my_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print (f'Key is {my_api_key}')\n",
    "\n",
    "client = OpenAI(api_key=my_api_key)\n",
    "\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful assistant. Summarize the user input and return as a JSON string.\"\n",
    "}\n",
    "\n",
    "# 2 --- Shared State ----\n",
    "class LibraryState(TypedDict):\n",
    "    question: Optional[str]\n",
    "    faq_answer: Optional[str]\n",
    "    checkout_info: Optional[str]\n",
    "    final_answer: Optional[str]\n",
    "\n",
    "def ClassifierAgent(state: LibraryState):\n",
    "    print(\"ClassifierAgent ran\")\n",
    "    print(f\"state: {state}\")\n",
    "    #return {}\n",
    "    \n",
    "    #build llm messages\n",
    "    message_to_llm = [\n",
    "        {\"role\": \"system\", \"content\": '''You are a classifier agent in a library system. \n",
    "        Decide if the user is asking about book availability/checkout or about library FAQs. \n",
    "        Reply with JSON containing keys: faq_answer and checkout_info.'''},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {state['question']}\"}\n",
    "    ]\n",
    "\n",
    "    #call llm the OpenAI model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=message_to_llm,\n",
    "        #temperature=0.7,\n",
    "        #max_tokens=250,\n",
    "    )\n",
    "    print(f\"response = {response}\")\n",
    "     # Extract the content from the response\n",
    "    answer = response.choices[0].message.content\n",
    "    # Ideally, parse as JSON — here assuming model returns a dict-like string\n",
    "    try:\n",
    "        import json\n",
    "        parsed = json.loads(answer)\n",
    "        print(f\"Raw response: {answer}\")\n",
    "        print(f\"Parsed response: {parsed}\")\n",
    "        return{\n",
    "            \"faq_answer\": parsed.get(\"faq_answer\", \"\"),\n",
    "            \"checkout_info\": parsed.get(\"checkout_info\", \"\")\n",
    "        }\n",
    "    except Exception:\n",
    "         # fallback if LLM gives plain text\n",
    "         return{\"faq_answer\": answer, \"checkout_info\": \"\"}\n",
    "\n",
    "def FAQAgent(state: LibraryState):\n",
    "    print(\"FAQAgent ran\")\n",
    "    print(f\"FAQAgent state\", state)\n",
    "    if not state.get(\"faq_answer\"):\n",
    "        return {\"faq_answer\": \"Default FAQ: Library rules apply\"}\n",
    "    return {\"faq_answer\": state[\"faq_answer\"]}\n",
    "\n",
    "def CheckoutAgent(state: LibraryState):\n",
    "    print(\"CheckoutAgent ran\")\n",
    "    if not state.get(\"checkout_info\"):\n",
    "        return {\"checkout_info\": \"Checkout info: Not requested\"}\n",
    "    return { \"checkout_info\": state[\"checkout_info\"]}\n",
    "\n",
    "def ResponseAgent(state: LibraryState):\n",
    "    print(\"ResponseAgent ran\")\n",
    "   \n",
    "\n",
    "    message_to_llm = [\n",
    "        {\"role\": \"system\", \"content\": '''You are a response builder for the library application.\n",
    "          Please combine the FAQ answer and checkout info into a coherent response to the user's question.\n",
    "         Return as a poem'''},\n",
    "         {\"role\": \"user\", \"content\": f\"FAQ: {state['faq_answer']}\\nCheckout Info: {state['checkout_info']}\\nQuestion: {state['question']}\"}\n",
    "    ]\n",
    "    # Call the OpenAI model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=message_to_llm,\n",
    "        # temperature=0.2,   # keep it deterministic for classification\n",
    "        # max_tokens=150,\n",
    "    )\n",
    "    print(response)\n",
    "    # Extract the content from the response\n",
    "    final_answer = response.choices[0].message.content\n",
    "\n",
    "\n",
    "    return {\"final_answer\": final_answer}\n",
    "\n",
    "\n",
    "# --- Build the Graph ---\n",
    "builder = StateGraph(LibraryState)\n",
    "builder.add_node(\"ClassifierAgent\", ClassifierAgent)\n",
    "builder.add_node(\"FAQAgent\", FAQAgent)\n",
    "builder.add_node(\"CheckoutAgent\", CheckoutAgent)\n",
    "builder.add_node(\"ResponseAgent\", ResponseAgent)\n",
    "\n",
    "builder.add_edge(START, \"ClassifierAgent\")\n",
    "builder.add_edge(\"ClassifierAgent\", \"FAQAgent\")\n",
    "builder.add_edge(\"ClassifierAgent\", \"CheckoutAgent\")\n",
    "builder.add_edge(\"FAQAgent\", \"ResponseAgent\")\n",
    "builder.add_edge(\"CheckoutAgent\", \"ResponseAgent\")\n",
    "builder.add_edge(\"ResponseAgent\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "result = graph.invoke({\"question\": \"When does library open\"})\n",
    "print(\"\\n--- Final Answer ---\")\n",
    "print(result[\"final_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f088e5a5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
